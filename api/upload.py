# -*- coding: utf-8 -*-
import pandas as pd
import re
import sqlite3
import logging
import os
import tempfile
from datetime import datetime
from flask import jsonify, request, g
from dbpy.database import get_db_connection, release_db_connection, calculate_record_hash
from utils.auth import token_required
from utils.operation_logger import log_operation
from utils.file_validator import FileValidator


def create_upload_logger(log_prefix="upload"):
    """
    ä¸ºä¸Šä¼ æ“ä½œåˆ›å»ºä¸“ç”¨çš„è°ƒè¯•æ—¥å¿—è®°å½•å™¨
    
    Args:
        log_prefix: æ—¥å¿—æ–‡ä»¶åçš„å‰ç¼€
        
    Returns:
        logger: é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨
        log_filename: ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    try:
        # ç¡®ä¿logsç›®å½•å­˜åœ¨
        os.makedirs('logs', exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"logs/{log_prefix}_debug_{timestamp}.log"
        
        # åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—è®°å½•å™¨
        logger_name = f"{log_prefix}_{timestamp}"
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.DEBUG)
        
        # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„æ—§å¤„ç†å™¨
        logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨ï¼ˆè¾“å‡ºåˆ°stdoutï¼‰
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # è®¾ç½®è¯¦ç»†çš„æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°è®°å½•å™¨
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        # è®°å½•åˆ›å»ºæ—¥å¿—è®°å½•å™¨çš„ä¿¡æ¯
        logger.info(f'åˆ›å»ºä¸Šä¼ è°ƒè¯•æ—¥å¿—è®°å½•å™¨: {logger_name}')
        logger.info(f'æ—¥å¿—æ–‡ä»¶: {log_filename}')
        
        return logger, log_filename
    except Exception as e:
        # å¦‚æœæ—¥å¿—è®°å½•å™¨åˆ›å»ºå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„è®°å½•å™¨ä½œä¸ºåå¤‡
        print(f"è­¦å‘Š: åˆ›å»ºä¸Šä¼ è°ƒè¯•æ—¥å¿—è®°å½•å™¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # åˆ›å»ºåŸºæœ¬çš„è®°å½•å™¨
        logger = logging.getLogger(f"{log_prefix}_fallback")
        logger.setLevel(logging.DEBUG)
        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)
        
        return logger, f"logs/{log_prefix}_fallback.log"


def log_upload_step(logger, step, message, level='info'):
    """
    è®°å½•ä¸Šä¼ è¿‡ç¨‹çš„æ­¥éª¤æ—¥å¿—
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨
        step: æ­¥éª¤åç§°
        message: æ—¥å¿—æ¶ˆæ¯
        level: æ—¥å¿—çº§åˆ« (debug, info, warning, error)
    """
    full_message = f"[{step}] {message}"
    if level == 'debug':
        logger.debug(full_message)
    elif level == 'warning':
        logger.warning(full_message)
    elif level == 'error':
        logger.error(full_message)
    else:  # info
        logger.info(full_message)


def register_upload_routes(app):
    """æ³¨å†Œä¸Šä¼ ç›¸å…³ API è·¯ç”±"""

    @app.route('/api/analyse/upload', methods=['POST'])
    @token_required
    def analyse_upload():
        """å¤„ç† Excel æ–‡ä»¶ä¸Šä¼ å¹¶ä¸Šä¼ åˆ°æ•°æ®åº“"""
        # åˆ›å»ºè°ƒè¯•æ—¥å¿—è®°å½•å™¨
        logger, log_filename = create_upload_logger("analyse_upload")
        logger.info('æ”¶åˆ° analyse æ–‡ä»¶ä¸Šä¼ è¯·æ±‚')
        
        try:
            # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
            current_user = g.current_user if hasattr(g, 'current_user') else None
            if current_user:
                logger.info(f'å½“å‰ç”¨æˆ·: {current_user.get("username", "æœªçŸ¥")}, è§’è‰²: {current_user.get("role", "æœªçŸ¥")}')
            
            if 'file' not in request.files:
                error_msg = 'é”™è¯¯ï¼šè¯·æ±‚ä¸­æ²¡æœ‰æ–‡ä»¶'
                logger.error(error_msg)
                return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400

            file = request.files['file']
            if file.filename == '':
                error_msg = 'é”™è¯¯ï¼šæ–‡ä»¶åä¸ºç©º'
                logger.error(error_msg)
                return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400

            logger.info(f'å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}')
            logger.info(f'æ–‡ä»¶å¤§å°: {len(file.read())} å­—èŠ‚')
            file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            
            # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•è¿›è¡ŒéªŒè¯
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp_file:
                file.save(tmp_file.name)
                logger.info(f'æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶ä½ç½®: {tmp_file.name}')
                
                # éªŒè¯æ–‡ä»¶æ ¼å¼
                logger.info('å¼€å§‹éªŒè¯æ–‡ä»¶æ ¼å¼...')
                is_valid, msg, df = FileValidator.validate_excel_format(tmp_file.name)
                
                if not is_valid:
                    os.unlink(tmp_file.name)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    logger.error(f'æ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥: {msg}')
                    return jsonify({'error': f'æ–‡ä»¶æ ¼å¼é”™è¯¯: {msg}'}), 400
                
                # éªŒè¯é€šè¿‡ï¼Œç»§ç»­å¤„ç†
                tmp_file_path = tmp_file.name
                logger.info('æ–‡ä»¶æ ¼å¼éªŒè¯é€šè¿‡')
            
            # ç»§ç»­ä¸Šä¼ å¤„ç†
            logger.info('å¼€å§‹ä¸Šä¼ å¤„ç†...')
            result = upload_to_database_internal_with_path(tmp_file_path, file.filename)
            logger.info(f'ä¸Šä¼ å¤„ç†å®Œæˆï¼Œç»“æœ: {result}')
            
            # è®°å½•æ“ä½œæ—¥å¿—
            if result.get('success') and current_user:
                log_operation(
                    username=current_user.get('username', 'unknown'),
                    role=current_user.get('role', 'user'),
                    operation_type='upload_excel_analyse',
                    detail={
                        'filename': file.filename,
                        'log_file': log_filename,
                        **result
                    },
                    result='success'
                )
            
            return result
        except Exception as e:
            error_msg = f'å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'
            logger.error(error_msg, exc_info=True)
            import traceback
            traceback.print_exc()
            
            # è®°å½•å¤±è´¥æ“ä½œæ—¥å¿—
            current_user = g.current_user if hasattr(g, 'current_user') else None
            if current_user:
                log_operation(
                    username=current_user.get('username', 'unknown'),
                    role=current_user.get('role', 'user'),
                    operation_type='upload_excel_analyse',
                    detail={'filename': file.filename if 'file' in locals() else 'unknown'},
                    result='failed',
                    error_message=str(e)
                )
            
            return jsonify({'error': str(e)}), 500

    @app.route('/api/upload', methods=['POST'])
    @token_required
    def upload_file():
        """å¤„ç†Excelæ–‡ä»¶ä¸Šä¼ """
        # åˆ›å»ºè°ƒè¯•æ—¥å¿—è®°å½•å™¨
        logger, log_filename = create_upload_logger("upload_file")
        logger.info('æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ è¯·æ±‚')
        
        try:
            # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
            current_user = g.current_user if hasattr(g, 'current_user') else None
            if current_user:
                logger.info(f'å½“å‰ç”¨æˆ·: {current_user.get("username", "æœªçŸ¥")}, è§’è‰²: {current_user.get("role", "æœªçŸ¥")}')
            
            if 'file' not in request.files:
                error_msg = 'é”™è¯¯ï¼šè¯·æ±‚ä¸­æ²¡æœ‰æ–‡ä»¶'
                logger.error(error_msg)
                return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400

            file = request.files['file']
            if file.filename == '':
                error_msg = 'é”™è¯¯ï¼šæ–‡ä»¶åä¸ºç©º'
                logger.error(error_msg)
                return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400

            logger.info(f'å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}')
            logger.info(f'æ–‡ä»¶å¤§å°: {len(file.read())} å­—èŠ‚')
            file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            
            # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•è¿›è¡ŒéªŒè¯
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp_file:
                file.save(tmp_file.name)
                logger.info(f'æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶ä½ç½®: {tmp_file.name}')
                logger.info(f'ä¸´æ—¶æ–‡ä»¶å¤§å°: {os.path.getsize(tmp_file.name)} å­—èŠ‚')
                
                # éªŒè¯æ–‡ä»¶æ ¼å¼
                logger.info('å¼€å§‹éªŒè¯æ–‡ä»¶æ ¼å¼...')
                is_valid, msg, df = FileValidator.validate_excel_format(tmp_file.name)
                
                if not is_valid:
                    os.unlink(tmp_file.name)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    logger.error(f'æ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥: {msg}')
                    return jsonify({'error': f'æ–‡ä»¶æ ¼å¼é”™è¯¯: {msg}'}), 400
                
                # éªŒè¯é€šè¿‡ï¼Œç»§ç»­å¤„ç†
                tmp_file_path = tmp_file.name
                logger.info('æ–‡ä»¶æ ¼å¼éªŒè¯é€šè¿‡')
                if df is not None:
                    logger.info(f'éªŒè¯æ—¶è¯»å–çš„æ•°æ®æ¡†: {len(df)} è¡Œ, {len(df.columns)} åˆ—')
                    logger.info(f'åˆ—å: {df.columns.tolist()}')
            
            # è¯»å–Excelæ–‡ä»¶
            logger.info('å¼€å§‹è¯»å–Excelæ–‡ä»¶...')
            df = pd.read_excel(tmp_file_path)
            logger.info(f'æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®ï¼Œ{len(df.columns)} åˆ—')
            logger.info(f'åˆ—å: {df.columns.tolist()}')
            logger.info(f'å‰å‡ è¡Œæ•°æ®æ ·æœ¬: {df.head(3).to_dict(orient="records") if not df.empty else "ç©ºæ•°æ®"}')

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_file_path)
            logger.info('å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶')

            # å¤„ç†æ•°æ®
            logger.info('å¼€å§‹å¤„ç†æ•°æ®...')
            result = process_data(df)
            logger.info(f'æ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(result["products"])} ä¸ªå•†å“')
            logger.info(f'å¤„ç†ç»“æœ: {result}')

            # è®°å½•æ“ä½œæ—¥å¿—
            if current_user:
                log_operation(
                    username=current_user.get('username', 'unknown'),
                    role=current_user.get('role', 'user'),
                    operation_type='upload_excel_analysis',
                    detail={
                        'filename': file.filename,
                        'log_file': log_filename,
                        'product_count': len(result["products"]),
                        'row_count': len(df)
                    },
                    result='success'
                )
            
            return jsonify(result)
        except Exception as e:
            error_msg = f'å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'
            logger.error(error_msg, exc_info=True)
            import traceback
            traceback.print_exc()
            
            # è®°å½•å¤±è´¥æ“ä½œæ—¥å¿—
            current_user = g.current_user if hasattr(g, 'current_user') else None
            if current_user:
                log_operation(
                    username=current_user.get('username', 'unknown'),
                    role=current_user.get('role', 'user'),
                    operation_type='upload_excel_analysis',
                    detail={'filename': file.filename if 'file' in locals() else 'unknown'},
                    result='failed',
                    error_message=str(e)
                )
            
            return jsonify({'error': str(e)}), 500
    
    # æ³¨å†Œåº“å­˜ä¸Šä¼ è·¯ç”±
    register_inventory_upload_routes(app)

    @app.route('/api/db/upload', methods=['POST'])
    @token_required
    def upload_to_database():
        """ä¸Šä¼ Excelæ•°æ®åˆ°æ•°æ®åº“"""
        # åˆ›å»ºè°ƒè¯•æ—¥å¿—è®°å½•å™¨
        logger, log_filename = create_upload_logger("upload_to_database")
        logger.info('æ”¶åˆ°æ•°æ®åº“ä¸Šä¼ è¯·æ±‚')
        
        try:
            # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
            current_user = g.current_user if hasattr(g, 'current_user') else None
            if current_user:
                logger.info(f'å½“å‰ç”¨æˆ·: {current_user.get("username", "æœªçŸ¥")}, è§’è‰²: {current_user.get("role", "æœªçŸ¥")}')
            
            if 'file' not in request.files:
                error_msg = 'é”™è¯¯ï¼šè¯·æ±‚ä¸­æ²¡æœ‰æ–‡ä»¶'
                logger.error(error_msg)
                return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400

            file = request.files['file']
            if file.filename == '':
                error_msg = 'é”™è¯¯ï¼šæ–‡ä»¶åä¸ºç©º'
                logger.error(error_msg)
                return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400

            logger.info(f'å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}')
            logger.info(f'æ–‡ä»¶å¤§å°: {len(file.read())} å­—èŠ‚')
            file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            
            # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•è¿›è¡ŒéªŒè¯
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp_file:
                file.save(tmp_file.name)
                logger.info(f'æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶ä½ç½®: {tmp_file.name}')
                logger.info(f'ä¸´æ—¶æ–‡ä»¶å¤§å°: {os.path.getsize(tmp_file.name)} å­—èŠ‚')
                
                # éªŒè¯æ–‡ä»¶æ ¼å¼
                logger.info('å¼€å§‹éªŒè¯æ–‡ä»¶æ ¼å¼...')
                is_valid, msg, df = FileValidator.validate_excel_format(tmp_file.name)
                
                if not is_valid:
                    os.unlink(tmp_file.name)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    logger.error(f'æ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥: {msg}')
                    return jsonify({'error': f'æ–‡ä»¶æ ¼å¼é”™è¯¯: {msg}'}), 400
                
                # éªŒè¯é€šè¿‡ï¼Œç»§ç»­å¤„ç†
                tmp_file_path = tmp_file.name
                logger.info('æ–‡ä»¶æ ¼å¼éªŒè¯é€šè¿‡')
                if df is not None:
                    logger.info(f'éªŒè¯æ—¶è¯»å–çš„æ•°æ®æ¡†: {len(df)} è¡Œ, {len(df.columns)} åˆ—')
                    logger.info(f'åˆ—å: {df.columns.tolist()}')
            
            logger.info('å¼€å§‹æ•°æ®åº“ä¸Šä¼ å¤„ç†...')
            result = upload_to_database_internal_with_path(tmp_file_path, file.filename)
            logger.info(f'æ•°æ®åº“ä¸Šä¼ å¤„ç†å®Œæˆï¼Œç»“æœ: {result}')
            
            # è®°å½•ä¸Šä¼ æ—¥å¿—
            if result.get('success'):
                detail = {
                    'filename': file.filename,
                    'log_file': log_filename,
                    'total': result.get('total', 0),
                    'success_count': result.get('success_count', 0),
                    'duplicate_count': result.get('duplicate_count', 0),
                    'error_count': result.get('error_count', 0),
                    'filtered_count': result.get('filtered_count', 0)
                }
                
                if current_user:
                    log_operation(
                        username=current_user.get('username', 'unknown'),
                        role=current_user.get('role', 'user'),
                        operation_type='upload_excel',
                        detail=detail,
                        result='success'
                    )
                
                # æ·»åŠ æ—¥å¿—æ–‡ä»¶è·¯å¾„åˆ°è¿”å›ç»“æœ
                result['debug_log'] = log_filename
            
            return result
        except Exception as e:
            error_msg = f'å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}'
            logger.error(error_msg, exc_info=True)
            import traceback
            traceback.print_exc()
            
            # è®°å½•å¤±è´¥æ—¥å¿—
            current_user = g.current_user if hasattr(g, 'current_user') else None
            if current_user:
                log_operation(
                    username=current_user.get('username', 'unknown'),
                    role=current_user.get('role', 'user'),
                    operation_type='upload_excel',
                    detail={'filename': file.filename if 'file' in locals() else 'unknown'},
                    result='failed',
                    error_message=str(e)
                )
            
            return jsonify({'error': str(e)}), 500


def process_data(df):
    """å¤„ç†Excelæ•°æ®ï¼Œç”Ÿæˆå•†å“é”€é‡ç»Ÿè®¡"""
    # å•†å“åç§°å»é‡ï¼šç§»é™¤é¢œè‰²ã€å°ºç ç­‰åç¼€ä¿¡æ¯
    def normalize_product_name(name):
        # ç§»é™¤å¸¸è§çš„é¢œè‰²ã€å°ºç åç¼€
        # ä¾‹å¦‚ï¼š--è“é©¬ç”², -58, 58CM, -XS, -61ç­‰
        name = str(name)
        # ç§»é™¤ -- åé¢çš„å†…å®¹
        name = re.sub(r'--.*', '', name)
        # ç§»é™¤ - åé¢è·Ÿç€æ•°å­—æˆ–å­—æ¯çš„å†…å®¹ï¼ˆå°ºç ï¼‰
        name = re.sub(r'-\s*\d+[A-Za-z]*', '', name)
        name = re.sub(r'-\s*[A-Za-z]+', '', name)
        # ç§»é™¤æœ«å°¾çš„æ•°å­—+å•ä½ï¼ˆå¦‚ 58CMï¼‰
        name = re.sub(r'\d+CM$', '', name)
        name = re.sub(r'\d+$', '', name)
        return name.strip()

    # æ ‡å‡†åŒ–å•†å“åç§°
    df['æ ‡å‡†åŒ–å•†å“åç§°'] = df['å•†å“åç§°'].apply(normalize_product_name)

    # è®¡ç®—é”€é‡ï¼šè®¢è´­æ•°å‡å»é€€æ¬¾æˆåŠŸçš„è®¢å•
    def calculate_sales(group):
        total = group['è®¢è´­æ•°'].sum()
        refunded = group[group['æ˜¯å¦é€€æ¬¾'] == 'é€€æ¬¾æˆåŠŸ']['è®¢è´­æ•°'].sum()
        return total - refunded

    # æŒ‰æ ‡å‡†åŒ–å•†å“åç§°åˆ†ç»„è®¡ç®—é”€é‡
    sales_data = df.groupby('æ ‡å‡†åŒ–å•†å“åç§°').apply(calculate_sales).reset_index()
    sales_data.columns = ['å•†å“åç§°', 'é”€é‡']

    # æŒ‰é”€é‡é™åºæ’åº
    sales_data = sales_data.sort_values('é”€é‡', ascending=False)

    # è½¬æ¢ä¸ºå‰ç«¯å¯ç”¨çš„æ ¼å¼
    result = {
        'products': sales_data['å•†å“åç§°'].tolist(),
        'sales': sales_data['é”€é‡'].tolist()
    }

    return result


def upload_to_database_internal(file):
    """å†…éƒ¨å‡½æ•°ï¼šä¸Šä¼ Excelæ•°æ®åˆ°æ•°æ®åº“ï¼ˆä¿æŒå…¼å®¹æ€§ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ–¹å¼ï¼‰"""
    print(f'å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}')

    # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•è¿›è¡Œå¤„ç†
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp_file:
        file.save(tmp_file.name)
        tmp_file_path = tmp_file.name

    # è°ƒç”¨è·¯å¾„ç‰ˆæœ¬çš„å¤„ç†å‡½æ•°
    return upload_to_database_internal_with_path(tmp_file_path, file.filename)


def upload_to_database_internal_with_path(file_path, original_filename):
    """å†…éƒ¨å‡½æ•°ï¼šä¸Šä¼ Excelæ•°æ®åˆ°æ•°æ®åº“ï¼ˆæ¥æ”¶æ–‡ä»¶è·¯å¾„ï¼‰"""
    # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿåˆ›å»ºè®°å½•å™¨
    logger, log_filename = create_upload_logger("upload_internal")
    logger.info(f'å¼€å§‹å¤„ç†æ–‡ä»¶: {original_filename}')
    logger.info(f'æ–‡ä»¶è·¯å¾„: {file_path}')
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(file_path):
        logger.error(f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return {
            'success': False,
            'error': f'ä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'
        }

    # è¯»å–Excelæ–‡ä»¶
    df = pd.read_excel(file_path)
    logger.info(f'æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®')

    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    os.unlink(file_path)

    # å°†æ‰€æœ‰Timestampç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    for col in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            df[col] = df[col].astype(str)
        # å°†NaTï¼ˆNot a Timeï¼‰è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        df[col] = df[col].where(pd.notna(df[col]), '')

    # åº”ç”¨å±‚å»é‡ï¼šå¤„ç†Excelæ–‡ä»¶å†…éƒ¨çš„é‡å¤
    df_deduped = df.drop_duplicates(keep='first')
    logger.info(f'Excelå†…å»é‡: {len(df)} -> {len(df_deduped)} æ¡è®°å½•')

    # è¿‡æ»¤æ‰åº—é“ºåç§°ä¸º"é‡‘è¶å¯¹æ¥"çš„è®°å½•
    df_filtered = df_deduped[df_deduped['åº—é“ºåç§°'] != 'é‡‘è¶å¯¹æ¥'].copy()
    logger.info(f'è¿‡æ»¤é‡‘è¶å¯¹æ¥è®°å½•: {len(df_deduped)} -> {len(df_filtered)} æ¡è®°å½•')

    if len(df_filtered) == 0:
        logger.info('è¿‡æ»¤åæ²¡æœ‰æ•°æ®å¯ä¸Šä¼ ')
        return {
            'success': True,
            'total': len(df_deduped),
            'success_count': 0,
            'duplicate_count': 0,
            'error_count': 0,
            'filtered_count': len(df_deduped)
        }

    # æ’å…¥æ•°æ®åº“
    conn = get_db_connection()
    cursor = conn.cursor()

    success_count = 0
    duplicate_count = 0
    error_count = 0

    # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
    from datetime import datetime
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    for idx, row in df_filtered.iterrows():
        try:
            # è®¡ç®—è®°å½•å“ˆå¸Œå€¼ç”¨äºå»é‡
            record_hash = calculate_record_hash(row)
            logger.info(f'å¤„ç†ç¬¬ {idx} è¡Œï¼Œè®¡ç®—çš„å“ˆå¸Œå€¼: {record_hash}')

            # æ’å…¥æ•°æ®åˆ°OrderDetailsè¡¨ï¼ˆä½¿ç”¨INSERT OR IGNOREæ¥é¿å…é‡å¤ï¼‰
            insert_sql = """
                INSERT OR IGNORE INTO OrderDetails (
                    record_hash, åº—é“ºç±»å‹, åº—é“ºåç§°, åˆ†é”€å•†åç§°, å•æ®ç¼–å·, è®¢å•ç±»å‹,
                    æ‹å•æ—¶é—´, ä»˜æ¬¾æ—¶é—´, å®¡æ ¸æ—¶é—´, ä¼šå‘˜ä»£ç , ä¼šå‘˜åç§°, å†…éƒ¨ä¾¿ç­¾, ä¸šåŠ¡å‘˜,
                    å»ºè®®ä»“åº“, å»ºè®®å¿«é€’, åˆ°è´¦, å•†å“å›¾ç‰‡, å“ç‰Œ, å•†å“ç¨ç‡, å•†å“ä»£ç ,
                    å•†å“åç§°, å•†å“ç®€ç§°, è§„æ ¼ä»£ç , è§„æ ¼åç§°, å•†å“å¤‡æ³¨, ä»£å‘è®¢å•, è®¢å•æ ‡è®°,
                    é¢„è®¡å‘è´§æ—¶é—´, è®¢è´­æ•°, æ€»é‡é‡, æŠ˜æ‰£, æ ‡å‡†è¿›ä»·, æ ‡å‡†å•ä»·, æ ‡å‡†é‡‘é¢,
                    å®é™…å•ä»·, å®é™…é‡‘é¢, è®©åˆ©åé‡‘é¢, è®©åˆ©é‡‘é¢, ç‰©æµè´¹ç”¨, æˆæœ¬æ€»ä»·,
                    ä¹°å®¶å¤‡æ³¨, å–å®¶å¤‡æ³¨, åˆ¶å•äºº, å•†å“å®é™…åˆ©æ¶¦, å•†å“æ ‡å‡†åˆ©æ¶¦, å•†å“å·²å‘è´§æ•°é‡,
                    å¹³å°æ——å¸œ, å‘è´§æ—¶é—´, åŸäº§åœ°, å¹³å°å•†å“åç§°, å¹³å°è§„æ ¼åç§°, ä¾›åº”å•†,
                    èµ å“æ¥æº, ä¹°å®¶æ”¯ä»˜é‡‘é¢, å¹³å°æ”¯ä»˜é‡‘é¢, å…¶ä»–æœåŠ¡è´¹, å‘ç¥¨ç§ç±»,
                    å‘ç¥¨æŠ¬å¤´ç±»å‹, å‘ç¥¨ç±»å‹, å¼€æˆ·è¡Œ, è´¦å·, å‘ç¥¨ç”µè¯, å‘ç¥¨åœ°å€, æ”¶è´§é‚®ç®±,
                    å‘¨æœŸè´­å•†å“, å¹³å°å•å·, åˆ°è´¦æ—¶é—´, é™„åŠ ä¿¡æ¯, å‘ç¥¨æŠ¬å¤´, å‘ç¥¨å†…å®¹,
                    çº³ç¨äººè¯†åˆ«å·, æ”¶è´§äºº, æ”¶è´§äººæ‰‹æœº, é‚®ç¼–, æ”¶è´§åœ°å€, å•†å“ç±»åˆ«,
                    äºŒæ¬¡å¤‡æ³¨, å•†å“å•ä½, å¸åˆ«, ä¼šå‘˜é‚®ç®±, è®¢å•æ ‡ç­¾, å¹³å°äº¤æ˜“çŠ¶æ€,
                    èµ å“, æ˜¯å¦é€€æ¬¾, åœ°åŒºä¿¡æ¯, ç¡®è®¤æ”¶è´§æ—¶é—´, ä½œåºŸ, åˆ›å»ºæ—¶é—´
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """

            # å‡†å¤‡æ•°æ®
            data = [
                record_hash,
                row.get('åº—é“ºç±»å‹', ''),
                row.get('åº—é“ºåç§°', ''),
                row.get('åˆ†é”€å•†åç§°', ''),
                row.get('å•æ®ç¼–å·', ''),
                row.get('è®¢å•ç±»å‹', ''),
                row.get('æ‹å•æ—¶é—´', ''),
                row.get('ä»˜æ¬¾æ—¶é—´', ''),
                row.get('å®¡æ ¸æ—¶é—´', ''),
                row.get('ä¼šå‘˜ä»£ç ', ''),
                row.get('ä¼šå‘˜åç§°', ''),
                row.get('å†…éƒ¨ä¾¿ç­¾', ''),
                row.get('ä¸šåŠ¡å‘˜', ''),
                row.get('å»ºè®®ä»“åº“', ''),
                row.get('å»ºè®®å¿«é€’', ''),
                row.get('åˆ°è´¦', ''),
                row.get('å•†å“å›¾ç‰‡', ''),
                row.get('å“ç‰Œ', ''),
                row.get('å•†å“ç¨ç‡', ''),
                row.get('å•†å“ä»£ç ', ''),
                row.get('å•†å“åç§°', ''),
                row.get('å•†å“ç®€ç§°', ''),
                row.get('è§„æ ¼ä»£ç ', ''),
                row.get('è§„æ ¼åç§°', ''),
                row.get('å•†å“å¤‡æ³¨', ''),
                row.get('ä»£å‘è®¢å•', ''),
                row.get('è®¢å•æ ‡è®°', ''),
                row.get('é¢„è®¡å‘è´§æ—¶é—´', ''),
                row.get('è®¢è´­æ•°', ''),
                row.get('æ€»é‡é‡', ''),
                row.get('æŠ˜æ‰£', ''),
                row.get('æ ‡å‡†è¿›ä»·', ''),
                row.get('æ ‡å‡†å•ä»·', ''),
                row.get('æ ‡å‡†é‡‘é¢', ''),
                row.get('å®é™…å•ä»·', ''),
                row.get('å®é™…é‡‘é¢', ''),
                row.get('è®©åˆ©åé‡‘é¢', ''),
                row.get('è®©åˆ©é‡‘é¢', ''),
                row.get('ç‰©æµè´¹ç”¨', ''),
                row.get('æˆæœ¬æ€»ä»·', ''),
                row.get('ä¹°å®¶å¤‡æ³¨', ''),
                row.get('å–å®¶å¤‡æ³¨', ''),
                row.get('åˆ¶å•äºº', ''),
                row.get('å•†å“å®é™…åˆ©æ¶¦', ''),
                row.get('å•†å“æ ‡å‡†åˆ©æ¶¦', ''),
                row.get('å•†å“å·²å‘è´§æ•°é‡', ''),
                row.get('å¹³å°æ——å¸œ', ''),
                row.get('å‘è´§æ—¶é—´', ''),
                row.get('åŸäº§åœ°', ''),
                row.get('å¹³å°å•†å“åç§°', ''),
                row.get('å¹³å°è§„æ ¼åç§°', ''),
                row.get('ä¾›åº”å•†', ''),
                row.get('èµ å“æ¥æº', ''),
                row.get('ä¹°å®¶æ”¯ä»˜é‡‘é¢', ''),
                row.get('å¹³å°æ”¯ä»˜é‡‘é¢', ''),
                row.get('å…¶ä»–æœåŠ¡è´¹', ''),
                row.get('å‘ç¥¨ç§ç±»', ''),
                row.get('å‘ç¥¨æŠ¬å¤´ç±»å‹', ''),
                row.get('å‘ç¥¨ç±»å‹', ''),
                row.get('å¼€æˆ·è¡Œ', ''),
                row.get('è´¦å·', ''),
                row.get('å‘ç¥¨ç”µè¯', ''),
                row.get('å‘ç¥¨åœ°å€', ''),
                row.get('æ”¶è´§é‚®ç®±', ''),
                row.get('å‘¨æœŸè´­å•†å“', ''),
                row.get('å¹³å°å•å·', ''),
                row.get('åˆ°è´¦æ—¶é—´', ''),
                row.get('é™„åŠ ä¿¡æ¯', ''),
                row.get('å‘ç¥¨æŠ¬å¤´', ''),
                row.get('å‘ç¥¨å†…å®¹', ''),
                row.get('çº³ç¨äººè¯†åˆ«å·', ''),
                row.get('æ”¶è´§äºº', ''),
                row.get('æ”¶è´§äººæ‰‹æœº', ''),
                row.get('é‚®ç¼–', ''),
                row.get('æ”¶è´§åœ°å€', ''),
                row.get('å•†å“ç±»åˆ«', ''),
                row.get('äºŒæ¬¡å¤‡æ³¨', ''),
                row.get('å•†å“å•ä½', ''),
                row.get('å¸åˆ«', ''),
                row.get('ä¼šå‘˜é‚®ç®±', ''),
                row.get('è®¢å•æ ‡ç­¾', ''),
                row.get('å¹³å°äº¤æ˜“çŠ¶æ€', ''),
                row.get('èµ å“', ''),
                row.get('æ˜¯å¦é€€æ¬¾', ''),
                row.get('åœ°åŒºä¿¡æ¯', ''),
                row.get('ç¡®è®¤æ”¶è´§æ—¶é—´', ''),
                row.get('ä½œåºŸ', ''),
                current_time  # æ·»åŠ åˆ›å»ºæ—¶é—´
            ]
            data = tuple(data)  # è½¬æ¢ä¸ºå…ƒç»„

            # æ‰§è¡Œæ’å…¥
            cursor.execute(insert_sql, data)

            # è®°å½•rowcountç”¨äºè°ƒè¯•
            rowcount = cursor.rowcount
            logger.info(f'ç¬¬ {idx} è¡Œæ’å…¥ç»“æœ: rowcount={rowcount}')
            
            if rowcount > 0:
                success_count += 1
                logger.info(f'ç¬¬ {idx} è¡Œ: æˆåŠŸæ’å…¥')
            else:
                duplicate_count += 1
                logger.info(f'ç¬¬ {idx} è¡Œ: é‡å¤è®°å½•ï¼ˆå·²å­˜åœ¨ï¼‰')

        except sqlite3.IntegrityError as e:
            # å¤„ç†æ•°æ®åº“å®Œæ•´æ€§é”™è¯¯ï¼ˆå¦‚å”¯ä¸€çº¦æŸè¿åï¼‰ï¼Œè®¡ä¸ºé‡å¤
            logger.error(f'ç¬¬ {idx} è¡Œå‘ç”Ÿå®Œæ•´æ€§é”™è¯¯: {e}')
            if "UNIQUE constraint failed" in str(e) or "PRIMARY KEY constraint failed" in str(e):
                duplicate_count += 1
                logger.info(f'ç¬¬ {idx} è¡Œ: æ£€æµ‹åˆ°å”¯ä¸€çº¦æŸè¿åï¼Œè®¡ä¸ºé‡å¤')
            else:
                logger.error(f'ç¬¬ {idx} è¡Œå‘ç”Ÿå…¶ä»–å®Œæ•´æ€§é”™è¯¯: {e}')
                error_count += 1
            continue
        except Exception as e:
            logger.error(f'æ’å…¥ç¬¬ {idx} è¡Œæ—¶å‡ºé”™: {e}')
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯è¿½è¸ª:\n{traceback.format_exc()}")
            error_count += 1
            continue

    # æäº¤äº‹åŠ¡
    conn.commit()

    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨"é‡‘è¶å¯¹æ¥"æ•°æ®
    cursor.execute("SELECT COUNT(*) FROM OrderDetails WHERE åº—é“ºåç§° = 'é‡‘è¶å¯¹æ¥'")
    jindie_count = cursor.fetchone()[0]

    conn.close()

    filtered_count = len(df_deduped) - len(df_filtered)

    print(f'ä¸Šä¼ å®Œæˆ: æˆåŠŸ={success_count}, é‡å¤={duplicate_count}, é”™è¯¯={error_count}, è¿‡æ»¤={filtered_count}')
    print(f'æ•°æ®åº“ä¸­"é‡‘è¶å¯¹æ¥"è®°å½•æ•°: {jindie_count}')

    result = {
        'success': True,
        'total': len(df_deduped),
        'success_count': success_count,
        'duplicate_count': duplicate_count,
        'error_count': error_count,
        'filtered_count': filtered_count
    }

    # å¦‚æœæ•°æ®åº“ä¸­å­˜åœ¨"é‡‘è¶å¯¹æ¥"æ•°æ®ï¼Œæ·»åŠ è­¦å‘Šä¿¡æ¯
    if jindie_count > 0:
        result['warning'] = f'æ•°æ®åº“ä¸­å­˜åœ¨ {jindie_count} æ¡"é‡‘è¶å¯¹æ¥"è®°å½•ï¼Œè¯·è”ç³»ç®¡ç†å‘˜å¤„ç†'

    return result


def register_inventory_upload_routes(app):
    """æ³¨å†Œåº“å­˜ä¸Šä¼ ç›¸å…³ API è·¯ç”±"""

    @app.route('/api/upload/inventory', methods=['POST'])
    @token_required
    def upload_inventory():
        """å¤„ç†åº“å­˜CSVæ–‡ä»¶ä¸Šä¼ """
        # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
        logger, log_filename = create_upload_logger("inventory_upload")
        logger.info('æ”¶åˆ°åº“å­˜æ–‡ä»¶ä¸Šä¼ è¯·æ±‚')
        logger.info(f'å½“å‰ç”¨æˆ·: {g.current_user["username"]}, è§’è‰²: {g.current_user["role"]}')

        if 'file' not in request.files:
            logger.error('é”™è¯¯ï¼šè¯·æ±‚ä¸­æ²¡æœ‰æ–‡ä»¶')
            return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400

        file = request.files['file']
        if file.filename == '':
            logger.error('é”™è¯¯ï¼šæ–‡ä»¶åä¸ºç©º')
            return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400

        logger.info(f'å¼€å§‹å¤„ç†åº“å­˜æ–‡ä»¶: {file.filename}')
        logger.info(f'æ–‡ä»¶å¤§å°: {file.content_length} å­—èŠ‚')

        try:
            # å¤„ç†åº“å­˜CSVæ–‡ä»¶ï¼Œä¼ é€’å½“å‰ç”¨æˆ·ä¿¡æ¯ç”¨äºè®°å½•æ“ä½œæ—¥å¿—
            result = process_inventory_csv(file, g.current_user)
            logger.info('åº“å­˜æ–‡ä»¶å¤„ç†å®Œæˆ')
            return result
        except Exception as e:
            logger.error(f'å¤„ç†åº“å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}')
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯è¿½è¸ª:\n{traceback.format_exc()}")
            return jsonify({'error': str(e)}), 500


def process_inventory_csv(file, current_user=None):
    """å¤„ç†åº“å­˜CSVæ•°æ®å¹¶æ’å…¥/æ›´æ–°åˆ°Inventoryè¡¨
    
    Args:
        file: ä¸Šä¼ çš„CSVæ–‡ä»¶å¯¹è±¡
        current_user: å½“å‰ç”¨æˆ·ä¿¡æ¯å­—å…¸ï¼ˆåŒ…å«usernameå’Œroleå­—æ®µï¼‰
    """
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger, log_filename = create_upload_logger("inventory_process")
    logger.info('å¼€å§‹å¤„ç†åº“å­˜CSVæ•°æ®')
    if current_user:
        logger.info(f'æ“ä½œç”¨æˆ·: {current_user.get("username", "unknown")}, è§’è‰²: {current_user.get("role", "unknown")}')
    
    # é¦–å…ˆéªŒè¯CSVæ–‡ä»¶æ ¼å¼ï¼Œä¼ é€’å¿…éœ€çš„åˆ—åˆ—è¡¨
    required_columns = ['å•†å“åç§°', 'ä»“åº“', 'æ•°é‡', 'å¯é”€æ•°', 'å¯é…æ•°', 'é”å®šæ•°', 'å•†å“å»ºæ¡£æ—¥æœŸ']
    logger.info(f'å¼€å§‹éªŒè¯CSVæ–‡ä»¶æ ¼å¼ï¼Œå¿…éœ€åˆ—: {required_columns}')
    is_valid, msg, df = FileValidator.validate_csv_format(file, required_columns)
    
    if not is_valid:
        logger.error(f'CSVæ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥: {msg}')
        return jsonify({'error': f'æ–‡ä»¶æ ¼å¼é”™è¯¯: {msg}'}), 400
    
    # ä½¿ç”¨éªŒè¯å‡½æ•°è¿”å›çš„DataFrameï¼Œé¿å…é‡å¤è¯»å–
    logger.info(f'CSVæ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œå…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—')
    logger.info(f'åˆ—å: {df.columns.tolist()}')
    logger.info(f'æ–‡ä»¶ç¼–ç : {msg.split("ç¼–ç : ")[-1] if "ç¼–ç :" in msg else "æœªçŸ¥"}')
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼ˆéªŒè¯å‡½æ•°å·²æ£€æŸ¥ï¼Œä½†å†æ¬¡ç¡®è®¤ï¼‰
    required_columns = ['å•†å“åç§°', 'ä»“åº“', 'æ•°é‡', 'å¯é”€æ•°', 'å¯é…æ•°', 'é”å®šæ•°', 'å•†å“å»ºæ¡£æ—¥æœŸ']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.error(f'CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}')
        return jsonify({'error': f'CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}'}), 400
    
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # è®°å½•æ•°æ®åº“å½“å‰çŠ¶æ€
    cursor.execute('SELECT COUNT(*) FROM Inventory')
    db_existing_count = cursor.fetchone()[0]
    logger.info(f'ğŸ“Š æ•°æ®åº“å½“å‰è®°å½•æ•°: {db_existing_count}')
    
    # ç»Ÿè®¡CSVä¸­çš„å”¯ä¸€è®°å½•æ•°ï¼ˆåŸºäºå•†å“åç§°+ä»“åº“ï¼‰
    unique_keys = df[['å•†å“åç§°', 'ä»“åº“']].drop_duplicates()
    csv_unique_count = len(unique_keys)
    logger.info(f'ğŸ“Š CSVæ–‡ä»¶å”¯ä¸€è®°å½•æ•°ï¼ˆå•†å“åç§°+ä»“åº“ï¼‰: {csv_unique_count}')
    logger.info(f'ğŸ“Š CSVæ–‡ä»¶æ€»è¡Œæ•°: {len(df)}')
    logger.info(f'ğŸ“Š CSVæ–‡ä»¶åˆ—æ•°: {len(df.columns)}')
    logger.info('=' * 60)
    
    # å®šä¹‰æ¸…ç†å‡½æ•°ï¼Œå»é™¤å­—ç¬¦ä¸²å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
    def clean_value(value):
        """æ¸…ç†å­—æ®µå€¼ï¼šå»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦ï¼Œä¿ç•™ä¸­é—´çš„ç©ºç™½"""
        if value is None:
            return None
        if isinstance(value, str):
            # å»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
            cleaned = value.strip()
            # å¦‚æœå»é™¤ç©ºç™½åå˜ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›None
            return cleaned if cleaned != '' else None
        # éå­—ç¬¦ä¸²å€¼ä¿æŒä¸å˜
        return value
    
    # å®šä¹‰å­—æ®µè½¬æ¢å‡½æ•°ï¼Œå¤„ç†å„ç§ç±»å‹çš„å­—æ®µè½¬æ¢
    def convert_field_value(value, field_type='str'):
        """è½¬æ¢å­—æ®µå€¼ï¼Œå¦‚æœclean_valueåä¸ºç©ºåˆ™è¿”å›None
        
        Args:
            value: åŸå§‹å€¼
            field_type: å­—æ®µç±»å‹ï¼Œå¯é€‰ 'str', 'int', 'float'
        Returns:
            è½¬æ¢åçš„å€¼ï¼Œå¦‚æœæ¸…ç†åä¸ºç©ºåˆ™è¿”å›None
        """
        # é¦–å…ˆæ¸…ç†å€¼
        cleaned = clean_value(value)
        if cleaned is None:
            return None
        
        # æ ¹æ®å­—æ®µç±»å‹è½¬æ¢
        try:
            if field_type == 'int':
                return int(cleaned)
            elif field_type == 'float':
                return float(cleaned)
            else:  # 'str' æˆ–å…¶ä»–ç±»å‹
                return cleaned
        except (ValueError, TypeError):
            # è½¬æ¢å¤±è´¥æ—¶è¿”å›None
            return None
    
    total_count = 0
    inserted_count = 0
    updated_count = 0
    failed_count = 0
    
    try:
        # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
        for index, row in df.iterrows():
            total_count += 1
            
            try:
                # æ¸…ç†å…³é”®å­—æ®µå€¼
                product_name = clean_value(row['å•†å“åç§°'])
                warehouse = clean_value(row['ä»“åº“'])
                
                # æ£€æŸ¥å•†å“åç§°å’Œä»“åº“æ˜¯å¦å·²å­˜åœ¨ï¼ˆä½¿ç”¨æ¸…ç†åçš„å€¼ï¼‰
                cursor.execute('''
                    SELECT id FROM Inventory 
                    WHERE å•†å“åç§° = ? AND ä»“åº“ = ?
                ''', (product_name, warehouse))
                
                existing_record = cursor.fetchone()
                
                if existing_record:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    update_sql = '''
                    UPDATE Inventory SET
                        æ•°é‡ = ?,
                        å¯é”€æ•° = ?,
                        å¯é…æ•° = ?,
                        é”å®šæ•° = ?,
                        å•†å“å»ºæ¡£æ—¥æœŸ = ?,
                        å•†å“ä»£ç  = ?,
                        å•†å“è§„æ ¼ä»£ç  = ?,
                        å•†å“è§„æ ¼åç§° = ?,
                        å•†å“æ ‡ç­¾ = ?,
                        å•†å“å•ä½ = ?,
                        åº“å­˜é‡é‡ = ?,
                        å¯é”€å”®å¤©æ•° = ?,
                        åœ¨é€”æ•° = ?,
                        å®‰å…¨åº“å­˜ä¸‹é™ = ?,
                        å®‰å…¨åº“å­˜ä¸Šé™ = ?,
                        è®¢å•å ç”¨æ•° = ?,
                        æœªä»˜æ¬¾æ•° = ?,
                        åº“ä½ = ?,
                        å•†å“æ¡ç  = ?,
                        å•†å“ç®€ç§° = ?,
                        å•†å“å¤‡æ³¨ = ?,
                        è§„æ ¼å¤‡æ³¨ = ?,
                        åº“å­˜çŠ¶æ€ = ?,
                        å•†å“åˆ†ç±» = ?,
                        å•†å“ç¨å· = ?,
                        ä¾›åº”å•† = ?,
                        ä¿è´¨æœŸ = ?,
                        æœ‰æ•ˆæ—¥æœŸ = ?,
                        ç”Ÿäº§æ—¥æœŸ = ?,
                        ä¾›åº”å•†è´§å· = ?,
                        å“ç‰Œ = ?,
                        ç®±è§„ = ?,
                        æ ‡å‡†è¿›ä»· = ?,
                        æœ€æ–°é‡‡è´­ä»· = ?,
                        æœ€æ–°é‡‡è´­ä¾›åº”å•† = ?,
                        æˆæœ¬ä»·æ ¼ = ?,
                        é”€å”®ä»·æ ¼ = ?,
                        æˆæœ¬æ€»é‡‘é¢ = ?,
                        é”€å”®æ€»é‡‘é¢ = ?,
                        è¿‘3æ—¥é”€é‡ = ?,
                        è¿‘7æ—¥é”€é‡ = ?,
                        è¿‘15æ—¥é”€é‡ = ?,
                        è¿‘30æ—¥é”€é‡ = ?,
                        æ›´æ–°æ—¶é—´ = CURRENT_TIMESTAMP
                    WHERE id = ?
                    '''
                    
                    # å‡†å¤‡æ›´æ–°æ•°æ®
                    update_data = (
                        convert_field_value(row['æ•°é‡'], 'int'),
                        convert_field_value(row['å¯é”€æ•°'], 'int'),
                        convert_field_value(row['å¯é…æ•°'], 'int'),
                        convert_field_value(row['é”å®šæ•°'], 'int'),
                        convert_field_value(row['å•†å“å»ºæ¡£æ—¥æœŸ'], 'str'),
                        convert_field_value(row.get('å•†å“ä»£ç '), 'str'),
                        convert_field_value(row.get('å•†å“è§„æ ¼ä»£ç '), 'str'),
                        convert_field_value(row.get('å•†å“è§„æ ¼åç§°'), 'str'),
                        convert_field_value(row.get('å•†å“æ ‡ç­¾'), 'str'),
                        convert_field_value(row.get('å•†å“å•ä½'), 'str'),
                        convert_field_value(row.get('åº“å­˜é‡é‡'), 'float'),
                        convert_field_value(row.get('å¯é”€å”®å¤©æ•°'), 'str'),
                        convert_field_value(row.get('åœ¨é€”æ•°'), 'int'),
                        convert_field_value(row.get('å®‰å…¨åº“å­˜ä¸‹é™'), 'int'),
                        convert_field_value(row.get('å®‰å…¨åº“å­˜ä¸Šé™'), 'int'),
                        convert_field_value(row.get('è®¢å•å ç”¨æ•°'), 'int'),
                        convert_field_value(row.get('æœªä»˜æ¬¾æ•°'), 'int'),
                        convert_field_value(row.get('åº“ä½'), 'str'),
                        convert_field_value(row.get('å•†å“æ¡ç '), 'str'),
                        convert_field_value(row.get('å•†å“ç®€ç§°'), 'str'),
                        convert_field_value(row.get('å•†å“å¤‡æ³¨'), 'str'),
                        convert_field_value(row.get('è§„æ ¼å¤‡æ³¨'), 'str'),
                        convert_field_value(row.get('åº“å­˜çŠ¶æ€'), 'str'),
                        convert_field_value(row.get('å•†å“åˆ†ç±»'), 'str'),
                        convert_field_value(row.get('å•†å“ç¨å·'), 'str'),
                        convert_field_value(row.get('ä¾›åº”å•†'), 'str'),
                        convert_field_value(row.get('ä¿è´¨æœŸ'), 'str'),
                        convert_field_value(row.get('æœ‰æ•ˆæ—¥æœŸ'), 'str'),
                        convert_field_value(row.get('ç”Ÿäº§æ—¥æœŸ'), 'str'),
                        convert_field_value(row.get('ä¾›åº”å•†è´§å·'), 'str'),
                        convert_field_value(row.get('å“ç‰Œ'), 'str'),
                        convert_field_value(row.get('ç®±è§„'), 'str'),
                        convert_field_value(row.get('æ ‡å‡†è¿›ä»·'), 'float'),
                        convert_field_value(row.get('æœ€æ–°é‡‡è´­ä»·'), 'float'),
                        convert_field_value(row.get('æœ€æ–°é‡‡è´­ä¾›åº”å•†'), 'str'),
                        convert_field_value(row.get('æˆæœ¬ä»·æ ¼'), 'float'),
                        convert_field_value(row.get('é”€å”®ä»·æ ¼'), 'float'),
                        convert_field_value(row.get('æˆæœ¬æ€»é‡‘é¢'), 'float'),
                        convert_field_value(row.get('é”€å”®æ€»é‡‘é¢'), 'float'),
                        convert_field_value(row.get('è¿‘3æ—¥é”€é‡'), 'int'),
                        convert_field_value(row.get('è¿‘7æ—¥é”€é‡'), 'int'),
                        convert_field_value(row.get('è¿‘15æ—¥é”€é‡'), 'int'),
                        convert_field_value(row.get('è¿‘30æ—¥é”€é‡'), 'int'),
                        existing_record[0]  # WHERE id = ?
                    )
                    
                    cursor.execute(update_sql, update_data)
                    updated_count += 1
                    print(f'âœ… ç¬¬ {index + 1} è¡Œ: æ›´æ–°è®°å½• (ID: {existing_record[0]}) å•†å“åç§°="{product_name}" ä»“åº“="{warehouse}"')
                    
                else:
                    # æ’å…¥æ–°è®°å½•
                    insert_sql = '''
                    INSERT INTO Inventory (
                        å•†å“åç§°, ä»“åº“, æ•°é‡, å¯é”€æ•°, å¯é…æ•°, é”å®šæ•°, å•†å“å»ºæ¡£æ—¥æœŸ,
                        å•†å“ä»£ç , å•†å“è§„æ ¼ä»£ç , å•†å“è§„æ ¼åç§°, å•†å“æ ‡ç­¾, å•†å“å•ä½,
                        åº“å­˜é‡é‡, å¯é”€å”®å¤©æ•°, åœ¨é€”æ•°, å®‰å…¨åº“å­˜ä¸‹é™, å®‰å…¨åº“å­˜ä¸Šé™,
                        è®¢å•å ç”¨æ•°, æœªä»˜æ¬¾æ•°, åº“ä½, å•†å“æ¡ç , å•†å“ç®€ç§°, å•†å“å¤‡æ³¨,
                        è§„æ ¼å¤‡æ³¨, åº“å­˜çŠ¶æ€, å•†å“åˆ†ç±», å•†å“ç¨å·, ä¾›åº”å•†, ä¿è´¨æœŸ,
                        æœ‰æ•ˆæ—¥æœŸ, ç”Ÿäº§æ—¥æœŸ, ä¾›åº”å•†è´§å·, å“ç‰Œ, ç®±è§„, æ ‡å‡†è¿›ä»·,
                        æœ€æ–°é‡‡è´­ä»·, æœ€æ–°é‡‡è´­ä¾›åº”å•†, æˆæœ¬ä»·æ ¼, é”€å”®ä»·æ ¼, æˆæœ¬æ€»é‡‘é¢,
                        é”€å”®æ€»é‡‘é¢, è¿‘3æ—¥é”€é‡, è¿‘7æ—¥é”€é‡, è¿‘15æ—¥é”€é‡, è¿‘30æ—¥é”€é‡
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 
                             ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 
                             ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    '''
                    
                    # å‡†å¤‡æ’å…¥æ•°æ®ï¼ˆå•†å“åç§°å’Œä»“åº“å·²ä½¿ç”¨clean_valueæ¸…ç†ï¼‰
                    insert_data = (
                        product_name,
                        warehouse,
                        convert_field_value(row['æ•°é‡'], 'int'),
                        convert_field_value(row['å¯é”€æ•°'], 'int'),
                        convert_field_value(row['å¯é…æ•°'], 'int'),
                        convert_field_value(row['é”å®šæ•°'], 'int'),
                        convert_field_value(row['å•†å“å»ºæ¡£æ—¥æœŸ'], 'str'),
                        convert_field_value(row.get('å•†å“ä»£ç '), 'str'),
                        convert_field_value(row.get('å•†å“è§„æ ¼ä»£ç '), 'str'),
                        convert_field_value(row.get('å•†å“è§„æ ¼åç§°'), 'str'),
                        convert_field_value(row.get('å•†å“æ ‡ç­¾'), 'str'),
                        convert_field_value(row.get('å•†å“å•ä½'), 'str'),
                        convert_field_value(row.get('åº“å­˜é‡é‡'), 'float'),
                        convert_field_value(row.get('å¯é”€å”®å¤©æ•°'), 'str'),
                        convert_field_value(row.get('åœ¨é€”æ•°'), 'int'),
                        convert_field_value(row.get('å®‰å…¨åº“å­˜ä¸‹é™'), 'int'),
                        convert_field_value(row.get('å®‰å…¨åº“å­˜ä¸Šé™'), 'int'),
                        convert_field_value(row.get('è®¢å•å ç”¨æ•°'), 'int'),
                        convert_field_value(row.get('æœªä»˜æ¬¾æ•°'), 'int'),
                        convert_field_value(row.get('åº“ä½'), 'str'),
                        convert_field_value(row.get('å•†å“æ¡ç '), 'str'),
                        convert_field_value(row.get('å•†å“ç®€ç§°'), 'str'),
                        convert_field_value(row.get('å•†å“å¤‡æ³¨'), 'str'),
                        convert_field_value(row.get('è§„æ ¼å¤‡æ³¨'), 'str'),
                        convert_field_value(row.get('åº“å­˜çŠ¶æ€'), 'str'),
                        convert_field_value(row.get('å•†å“åˆ†ç±»'), 'str'),
                        convert_field_value(row.get('å•†å“ç¨å·'), 'str'),
                        convert_field_value(row.get('ä¾›åº”å•†'), 'str'),
                        convert_field_value(row.get('ä¿è´¨æœŸ'), 'str'),
                        convert_field_value(row.get('æœ‰æ•ˆæ—¥æœŸ'), 'str'),
                        convert_field_value(row.get('ç”Ÿäº§æ—¥æœŸ'), 'str'),
                        convert_field_value(row.get('ä¾›åº”å•†è´§å·'), 'str'),
                        convert_field_value(row.get('å“ç‰Œ'), 'str'),
                        convert_field_value(row.get('ç®±è§„'), 'str'),
                        convert_field_value(row.get('æ ‡å‡†è¿›ä»·'), 'float'),
                        convert_field_value(row.get('æœ€æ–°é‡‡è´­ä»·'), 'float'),
                        convert_field_value(row.get('æœ€æ–°é‡‡è´­ä¾›åº”å•†'), 'str'),
                        convert_field_value(row.get('æˆæœ¬ä»·æ ¼'), 'float'),
                        convert_field_value(row.get('é”€å”®ä»·æ ¼'), 'float'),
                        convert_field_value(row.get('æˆæœ¬æ€»é‡‘é¢'), 'float'),
                        convert_field_value(row.get('é”€å”®æ€»é‡‘é¢'), 'float'),
                        convert_field_value(row.get('è¿‘3æ—¥é”€é‡'), 'int'),
                        convert_field_value(row.get('è¿‘7æ—¥é”€é‡'), 'int'),
                        convert_field_value(row.get('è¿‘15æ—¥é”€é‡'), 'int'),
                        convert_field_value(row.get('è¿‘30æ—¥é”€é‡'), 'int')
                    )
                    
                    cursor.execute(insert_sql, insert_data)
                    inserted_count += 1
                    print(f'âœ… ç¬¬ {index + 1} è¡Œ: æ’å…¥æ–°è®°å½• å•†å“åç§°="{product_name}" ä»“åº“="{warehouse}"')
                    
            except Exception as e:
                print(f'âŒ å¤„ç†ç¬¬ {index + 1} è¡Œæ—¶å‡ºé”™: {e}')
                print(f'   é—®é¢˜æ•°æ®: å•†å“åç§°="{product_name}", ä»“åº“="{warehouse}"')
                import traceback
                traceback.print_exc()
                failed_count += 1
                continue
        
        conn.commit()
        
        # æŸ¥è¯¢æœ€ç»ˆæ•°æ®åº“è®°å½•æ•°
        cursor.execute('SELECT COUNT(*) FROM Inventory')
        db_final_count = cursor.fetchone()[0]
        print(f'ğŸ“Š æ•°æ®åº“æœ€ç»ˆè®°å½•æ•°: {db_final_count}')
        print(f'ğŸ“Š æ•°æ®åº“è®°å½•å˜åŒ–: +{inserted_count}æ–°å¢, {updated_count}æ›´æ–°')
        print('=' * 60)
        
        # è®°å½•æ“ä½œæ—¥å¿—
        from utils.operation_logger import log_operation
        if current_user:
            log_operation(current_user['username'], current_user['role'], 'upload_inventory', 
                         f'ä¸Šä¼ åº“å­˜æ•°æ®: æ€»è®¡{total_count}è¡Œ, æ–°å¢{inserted_count}è¡Œ, æ›´æ–°{updated_count}è¡Œ, å¤±è´¥{failed_count}è¡Œ')
        else:
            print('è­¦å‘Šï¼šcurrent_userä¸ºç©ºï¼Œè·³è¿‡æ“ä½œæ—¥å¿—è®°å½•')
        
        print(f'åº“å­˜ä¸Šä¼ å®Œæˆ: æ€»è®¡{total_count}è¡Œ, æ–°å¢{inserted_count}è¡Œ, æ›´æ–°{updated_count}è¡Œ, å¤±è´¥{failed_count}è¡Œ')
        
        return jsonify({
            'success': True,
            'total': total_count,
            'inserted': inserted_count,
            'updated': updated_count,
            'failed': failed_count,
            'message': 'åº“å­˜æ•°æ®ä¸Šä¼ å®Œæˆ'
        })
        
    except Exception as e:
        conn.rollback()
        print(f'å¤„ç†åº“å­˜æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}')
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'å¤„ç†åº“å­˜æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'}), 500
    finally:
        conn.close()